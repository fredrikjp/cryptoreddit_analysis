name: Run Kafka Stream Every Hour

on:
  schedule:
    - cron: '0 * * * *'  # Every hour at minute 0
  workflow_dispatch:     # Allow manual triggering
  push:               # Trigger on push events

jobs:
  run-kafka-stream:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python and install dependencies
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'  # or whatever version you use

      - name: Install Python requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
        # ðŸ”„ Restore previous run data
      - name: Download previous run data
        uses: actions/download-artifact@v4
        with:
          name: run-data
          path: consumer/data/
        continue-on-error: true  # In case it's the first run

      - name: Set up Docker Compose
        run: |
          docker compose up -d
          echo "Kafka + Zookeeper starting up..."
          sleep 30  # Give services time to initialize

      - name: Run Producer (Reddit Stream)
        run: |
          echo "Starting producer..."
          timeout 120 python producer/reddit_stream_producer.py
        continue-on-error: true

      - name: Run Consumer (Sentiment Consumer)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          echo "Starting consumer..."
          timeout 120 python consumer/sentiment_consumer.py
        continue-on-error: true

      - name: Shut down Docker Compose
        run: docker compose down

        # ðŸ’¾ Upload current run data
      - name: Upload run data for next job
        uses: actions/upload-artifact@v4
        with:
          name: run-data
          path: consumer/data/
